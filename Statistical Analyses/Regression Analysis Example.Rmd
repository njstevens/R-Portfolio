---
title: "MATH 425 Project 1"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
---

```{r, include=FALSE}
library(mosaic)
library(ggthemes)
library(DT)
library(pander)
library(ggplot2)
library(dplyr)
library(tidyverse)
```

## Background

This analysis will focus on data that was gathered from utility bills at an unknown residence (see table below).

```{r, echo=TRUE, warning = FALSE, comment = NA}
datatable(Utilities, options=list(lengthMenu = c(3,10,30)))
```

There are many different variables to analyze, but the variables of concern for this analysis are the average temperature during the billing period (temp) and the total bill for that particular residence (total bill).  The goal here is to see if there is a linear relationship between the average temperature and the total bill, i.e. to see if temperature acts as a reliable predictor for a residents total bill.  A linear regression analysis with the model of the form $Y_i=\beta_0+\beta_1X_i+\epsilon_i$ will be used.

To conclude whether or not there is a linear relationship between the two variables, the following hypotheses test must be conducted.

$$
H_0: \beta_1=0 \\
H_1: \beta_1 \ne0\\
\alpha = 0.05
$$
If a low enough p-value is obtained, and the null hypothesis rejected, it can be concluded that there is a linear relationship between the two variables.

## Linear Regression Analysis

To gain a visual understanding of the "temp"" and "totalbill" variables, a scatter plot was created.  Note that the data points turn Green as the price gets lower.
```{r, echo=TRUE, warning = FALSE, comment = NA}
Utilities %>%
  ggplot()+
  geom_point(aes(x = temp, y = totalbill, col = totalbill))+
  scale_colour_gradient(aes(x = temp, y = totalbill), low = "green", high = "red")+
  labs(title = "Money Spent Based on Temperature", x = "Temperature", y = "Total Bill")+
  theme(legend.position = "none")

```


It is obvious that there is some kind of trend between the two variables, but to test this the following linear regression analysis results were obtained.
```{r, echo=TRUE, warning = FALSE, comment = NA}
ut.lm<-lm(totalbill~temp, data=Utilities)
pander(summary(ut.lm))

```

Fortunately, the p value is well below $\alpha$, and the Null hypothesis is rejected.  Since it is determined that there is a linear relationship between the two variables, the equation of the regression line is as follows:

$$
\hat{Y}_i=292.2 - 2.762X_i
$$
And below is the regression line plotted with the data.
```{r, echo=TRUE, warning = FALSE, comment = NA}
Utilities %>%
  ggplot()+
  geom_point(aes(x = temp, y = totalbill, col = totalbill))+
  geom_smooth(aes(x = temp, y = totalbill), method='lm', col ="black", se = F, size = 0.4)+
  scale_colour_gradient(aes(x = temp, y = totalbill), low = "green", high = "red")+
  labs(title = "Money Spent Based on Temperature", x = "Temperature", y = "Total Bill")+
  theme(legend.position = "none")

```

Seeing that the p-value for the slope (temp) is significantly below the $\alpha=0.05$ level of significance, it can be concluded that there is a linear relationship between the two variables.  Also, the correlation coeficient measures at about 0.674, which means the data are somewhat tightly fitted to the line.

### Interpretation and Diagnostics of the Model

However, just because there is a linear relationship, and a decently strong correlation does not necessarily mean that a linear regression model was the best fit for the data.  To interpret the model, the following assumptions must be checked in order to determine the appropriateness of the model.

#### {.tabset .tabset-pills .tabset-fade}

##### Linearity and Constant Variance of Errors

The plot below shows a plot of the residuals and the fitted values ($\hat{Y}_i$'s).  From just looking at the picture, it could be concluded that linearity is an issue.  However, there does not seem to be much of an issue with the variance. A simple remedy to account for this in the model would be to perform a transformation of the data to get a regression line that fits the data better.  A transformation such that a curve would fit better rather than a line.  Given that there does not need seem to be any issues with the variance, a transformation of the X variable (temp) is only needed.  See "Transformation of the Model" for more on this remedy.

```{r, echo=TRUE, warning=FALSE, comment=NA}
plot(ut.lm, which=1)
```

##### Presence of Outliers

The box plot below shows any presence of outliers within the residuals of the data.  In this particular case, there does not seem to be any outliers that have a critical effect on the regression line.

```{r, echo=TRUE, warning=FALSE, comment=NA}
boxplot(ut.lm$residuals, col="darkslategray4", main="No Obvious Outliers")
```

##### Nonindependence of Terms

The sequence plot below shows how independent the errors are from each other.  Independence is determined by any seeing if the residuals fluctuate around a baseline of 0 (red line).  Fortunately there does not seem to be much of a pattern, and it can be concluded that the error terms are independent.

```{r, echo=TRUE, warning=FALSE, comment=NA}
plot(ut.lm$residuals, type='b', pch=16, cex=0.8, main="Sequence Plot of Residuals")
abline(h=0, col="red")
```

##### Normality of Error Terms

The normality plot below shows how normally distributed the error terms are.  The confidence band (red dashed lines) provided acts as a point of reference to help test normality.  Seeing as all terms are inside the red lines, it can be concluded that the resdiuals are normally distributed.

```{r, echo=TRUE, warning=FALSE, comment=NA}
plot(ut.lm, which=2)
```

##### Omission of Important Predictor Variables

There are several different possible predictor variables that could factor into having an effect on the total bill.  One of particular interest is the kwh variable.  The plot below shows the trend that the kwh variable shows.  This will not be factored into this analysis, but it is definitley worth considering for future analyses.

```{r, echo=TRUE, warning=FALSE, comment=NA}
plot(ut.lm$residuals~Utilities$kwh, pch=16, col="firebrick", main="Trend of kWh")
```


### Transformation of the Model

In the "Linearity and Constant Variance of Errors" tab, a residuals vs fitted values plot was given, and it was determined that there were issues with the Linearity.  It was also determined that a transformation on the X variable only would be necessary.  However, it is wise to run a lack of fit F test to be sure.  

##### Lack of Fit Test

The following hypotheses have to be made in order to perform the lack of fit F test.
$$
H_0: E{\{Y\}}=\beta_0+\beta_1X \\
H_1: E{\{Y\}}\ne\beta_0+\beta_1X\\
\alpha=0.05
$$

Though Linear regression is shown to be appropriate, a transformation on the X variable still yields an equation that will be a better fit.  However, it is important to note here, that transformations are only useful when it comes to making prediction and a transformed model cannot be interpreted.

##### Transformation

Based off of prototype regression patterns, it was determined that a transformation of x by $log_{10}$ would be the most appropriate.  Below are the results of the newly transformed data.

```{r, echo=TRUE, warning=FALSE, comment=NA}
ut.log.lm<-lm(totalbill~log10(temp), data = Utilities)
pander(summary(ut.log.lm))
```

This would make the new regression line equation:

$$
\hat{Y}_i= 578.2 - 257*log_{10}(x)
$$

Below is the graph of the newly transformed line to fit the data.  Note that the color changes to purple as the price goes down (the change in colors of this data set is different because this represents a transformed model, not transformed data).  Plotted with the curved blue line (the model) is a red Lowess curve.  The Lowess curve uses the median values from neighborhoods around X, rather than using the average as the model above uses.  Doing this gives a non paramtric perspective to the model.

```{r, echo=TRUE, warning=FALSE, comment=NA}
Utilities %>%
  ggplot()+
  geom_point(aes(x = temp, y = totalbill, col = totalbill))+
  geom_smooth(aes(x = temp, y = totalbill), method = "lm", formula = y ~log10(x), col ="black", se = F, size = 0.4)+
  geom_smooth(aes(x=temp, y = totalbill), method = "loess", col = "red", se = F, size =0.2)+
  scale_colour_gradient(aes(x = temp, y = totalbill), low = "green", high = "red")+
  labs(title = "Money Spent Based on Temperature", x = "Temperature", y = "Total Bill")+
  theme(legend.position = "none")
  
```


## Conclusion

Even though it was determined that linear regression is appropriate for the data, there was still room for improvement on when it came to prediction.  Notice how the curved line fits the data a lot better than just the simple line.  As stated before, the transformation only works for prediction, so below are prediction comparisons of both the original linear model and the transformed curved model.

When the average temperature over a billing period is $X=100$, the total bill price to be expected is:

Linear Model:

$$
\hat{Y}_i=292.2 - 2.762(100)= 15.80
$$

Transformed Model:

$$
\hat{Y}_i= 578.2 - 257*log_{10}(100)= 64.2
$$

We can also create a prediction interval for both the original and transformed models.  Sigma is estimated by the Residual Standard Error to be $\sigma=39.45 $, and we can calculate the 95% prediction interval for the original model when $X_h=100$ to be approximately $15.80\pm 1.980808(39.45)$, which yields an interval of $-62.34\le Y_{h(new)} \le 93.94$.  However, a computer analysis showed a more accurate prediction interval as seen below.

```{r}
pander(predict(ut.lm, data.frame(temp = 100), interval="prediction"))
```

For the transformed model a prediction interval can also be determined, however the standard deviation needs to be transformed to accomodate the transformed model.  Thus the standard deviation of the temperature of the transformed model where the Residual Standard Error estimates sigma to be $\sigma=35.72 $. This now yeilds a 95% prediction interval for when $X_h=100$ that is $64.2\pm 1.980808(35.72)$ which is approximately $-6.554\le Y_{h(new)} \le 134.954$. Again, a computer analysis gave a more accurate prediction interval that can be seen below.

```{r}
pander(predict(ut.log.lm, data.frame(temp = 100), interval="prediction"))
```

Confidence intervals can also be calculated for the values at $X_h=100$.  Given that the original model has an $s[\hat{Y}_h]=9.87372$, we can calculate the 95% confidence interval of the original model to be approximately $-3.758\le E[Y_{h}] \le 35.358$.  Below are the more accurate results of a computer analysis.

```{r}
pander(predict(ut.lm, data.frame(temp = 100), interval="confidence"))
```

For the transformed model, $s[\hat{Y}_h]= 8.91308$ which yields a 95% confidence interval of approximately $46.544\le E[Y_{h}] \le 81.855$.  This confidence interval had to account for the transformation on X, hence why the $s[\hat{Y}_h]$ is different.  Below are the results of the computer analysis for the confidence interval.

```{r}
pander(predict(ut.log.lm, data.frame(temp = 100), interval="confidence"))
```

Ultimately, there is a significant difference in predictions, as well as the confidence and prediction intervals, but the transformed model creates a more accurate prediction becuase the data fit closer to that line, and thus the trend will follow closer to it.  This can be confirmed by the $R^2$ coefficient in the transformed model. The results where $R^2 = 0.7325$ which indicates a strong correlation, or a tighter fit to the line.

This is a stronger correlation than the original models $R^2$, which yeilded $R^2 = 0.6738$.  Both models do well when in determining an expected total bill, but the transformed model seems to be a more trustworthy source when it comes to prediction.

Thus the conclusion to this analysis is to use the transformed model:
$$
\hat{Y}_i= 578.2 - 257*log_{10}(x)
$$
for predictions when trying to calculate an expected total bill.
